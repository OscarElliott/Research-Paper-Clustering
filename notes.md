# General Notes
- 54 and 71 are links to the same article
- I implemented a remove duplicates function to tackle this type of issue
- Issue with legal grey area web scrapping - through simulated user - appears to be legal as I am not doing anything with the scrapped data but still against TOS
- the difference between using a principal componet dimensional reduction of 5 vs 6 is the difference between keeping vs having noise points some of them seemed to be unrelated while some should have been in clusters that they are not (from a human perspective)
- choice to use SentenceTransformer over alternatives such as TF-IDF as it is a better choice for understanding the context of the text 
and semantics.
- DBSCAN